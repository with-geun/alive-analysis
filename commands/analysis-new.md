# /analysis new

Start a new analysis. Supports `--from {Quick ID}` for Quick-to-Full succession.

## Instructions

### Step 1: Check initialization

Verify `.analysis/config.md` exists. If not, tell the user to run `/analysis init` first.
Read config.md to load team context (metrics, stakeholders, data stack).

### Step 2: Parse arguments

- If `--from {ID}` is provided, this is a Quickâ†’Full succession (skip to Step 2c)
- Otherwise, proceed to Step 2a

### Step 2a: Ask analysis type

Use AskUserQuestion:

**Q1. What type of analysis?**

- ðŸ” **Investigation** â€” "Why did this happen?" Root cause analysis, anomaly detection, ad-hoc deep dives.
- ðŸ“ˆ **Modeling** â€” "Can we predict/classify/segment?" Statistical modeling, ML, forecasting.

(Phase 2 types â€” not yet available:)
- ðŸ“Š Monitoring â€” periodic metric health check
- ðŸ§ª Experiment â€” A/B test analysis

### Step 2b: Ask mode and details

**Q2. Analysis mode?**
- **Full** â€” 5 ALIVE files + full checklists.
- **Quick** â€” Single file with abbreviated ALIVE flow.

**Q3. Analysis title?**
- Free text input

**Q4. (Full only) Brief description**
- Investigation: "What do you want to find out?"
- Modeling: "What do you want to predict or classify?"
- This seeds the ASK phase.

### Step 2c: Quickâ†’Full succession

- Read the Quick file referenced by `--from {ID}`
- Extract content from each ALIVE section
- Use as seed content for Full analysis files
- Add a reference link in the Quick file: `> â¬†ï¸ Promoted to Full: {new Full ID}`

### Step 3: Generate ID

Read `.analysis/status.md` to determine the next sequence number for today.

- Full: `F-{YYYY}-{MMDD}-{seq}` (e.g., `F-2026-0210-001`)
- Quick: `Q-{YYYY}-{MMDD}-{seq}` (e.g., `Q-2026-0210-002`)

Sequence resets daily, starts at 001.

### Step 4: Create files

---

#### 4A. Full Investigation

Create folder: `analyses/active/{ID}_{title-slug}/`

Generate `01_ask.md`:
```markdown
# ASK: {title}
> ID: {ID} | Type: ðŸ” Investigation | Stage: â“ ASK | Started: {YYYY-MM-DD}

## Problem Definition
- Question:
- Requester:
- Background:
- Trigger: What event/observation prompted this question?

## Framing
- Type: Causation ("Why did X happen?") / Correlation ("Are X and Y related?")
- Decision this will inform:
- Cost of being wrong:

## Hypothesis Tree
```
Main question: "{title}"
â”œâ”€â”€ Internal factors
â”‚   â”œâ”€â”€ Product changes:
â”‚   â”œâ”€â”€ Channel/acquisition changes:
â”‚   â”œâ”€â”€ Cross-service impact:
â”‚   â””â”€â”€ Operations/pricing changes:
â”œâ”€â”€ External factors
â”‚   â”œâ”€â”€ Seasonality/calendar:
â”‚   â”œâ”€â”€ Competitor actions:
â”‚   â”œâ”€â”€ Market/economic shifts:
â”‚   â””â”€â”€ Platform changes:
â””â”€â”€ Data artifacts
    â”œâ”€â”€ Tracking/instrumentation changes:
    â”œâ”€â”€ Metric definition changes:
    â””â”€â”€ Population/mix changes:
```

## Success Criteria
- What does "done" look like?

## Assumptions
-

## Scope
- In scope:
- Out of scope:
- Deadline:
- Multi-lens plan: macro (market) / meso (company) / micro (user)

## Data Sources
- Primary:
- Secondary:
- Access method: (MCP / file / manual query / BI dashboard)

---
{Insert ASK checklist from .analysis/checklists/ask.md}
```

Generate `assets/README.md`:
```markdown
# Assets: {title}
> Analysis ID: {ID}

## Contents
(Describe your SQL files, notebooks, charts, and other artifacts here)

## Key Files
- (add as you work)
```

---

#### 4B. Full Modeling

Create folder: `analyses/active/{ID}_{title-slug}/`

Generate `01_ask.md`:
```markdown
# ASK: {title}
> ID: {ID} | Type: ðŸ“ˆ Modeling | Stage: â“ ASK | Started: {YYYY-MM-DD}

## Objective
- What are you trying to predict / classify / segment?
- Business impact: Why does this matter?
- Decision this model will inform:
- What happens if the model is wrong? (cost of false positives vs false negatives)

## Success Criteria
- Target metric (e.g., "AUC > 0.8", "MAPE < 10%")
- Business success (e.g., "reduce churn by 5% if deployed")
- Minimum viable performance (below which the model isn't useful):

## Assumptions
-

## Scope
- Target variable:
- Candidate features:
- Training period:
- Prediction horizon:
- Expected deployment context (batch / real-time / on-demand):

## Data Sources
- Primary:
- Secondary:
- External data opportunities: (market data, third-party signals)
- Cross-service features available:
- Access method: (MCP / file / manual query)

## Constraints
- Must be interpretable? (Yes / No â€” and for whom?)
- Real-time requirement? (Yes / No â€” latency budget?)
- Fairness constraints: (protected groups, bias concerns)
- Guardrail metrics to watch: (reference from config.md)

---
{Insert ASK checklist from .analysis/checklists/ask.md}
```

Generate `assets/README.md` (same as Investigation).

**Modeling-specific stage templates** (generated by `/analysis next`):

**02_look.md for Modeling:**
```markdown
# LOOK: {title}
> ID: {ID} | Type: ðŸ“ˆ Modeling | Stage: ðŸ‘€ LOOK | Updated: {YYYY-MM-DD}

## Target Variable
- Distribution:
- Class balance (if classification):
- Missing rate:
- Temporal patterns:

## Feature Exploration
| Feature | Type | Missing % | Correlation w/ Target | Leakage Risk | Notes |
|---------|------|-----------|----------------------|-------------|-------|
| | | | | | |

## Data Quality
- Outliers:
- Leakage risk (features that contain future information):
- Time-based splits needed?
- Feature staleness (how fresh does each feature need to be?):

## External & Cross-Service Features
- External data sources worth including? (market, weather, competitor)
- Cross-service features available? (from adjacent products/services)
- Feature engineering opportunities:

## Confounding & Bias Check
- Selection bias in training data?
- Label quality (how reliable is the target variable?):
- Population drift risk (will the training population match production?):

## Sampling Strategy
- Training / Validation / Test split:
- Stratification:
- Temporal ordering preserved?

---
{Insert LOOK checklist}
```

**03_investigate.md for Modeling:**
```markdown
# INVESTIGATE: {title}
> ID: {ID} | Type: ðŸ“ˆ Modeling | Stage: ðŸ” INVESTIGATE | Updated: {YYYY-MM-DD}

## Baseline
- Naive baseline method:
- Baseline performance:
- Why this baseline? (justification):

## Model Selection
| Model | Hyperparameters | Train Score | Val Score | Test Score | Overfit? | Notes |
|-------|----------------|-------------|-----------|-----------|---------|-------|
| | | | | | | |

## Best Model
- Model:
- Key features (importance / coefficients):
- Performance vs success criteria:
- Why this model over alternatives? (interpretability, speed, accuracy trade-off):

## Validation
- Cross-validation results:
- Test set performance:
- Overfitting check (train vs val gap):
- Temporal validation (if time-series): does performance degrade over time?

## Sensitivity Analysis
- Feature ablation: which features can be removed without significant loss?
- Hyperparameter sensitivity: how much does performance change with different params?
- Data size sensitivity: how does performance change with less training data?

## Error Analysis
- Where does the model fail? (which segments, which examples):
- Systematic bias? (does it consistently over/under-predict for certain groups):
- Fairness check: different performance across demographic segments?
- Edge cases: extreme values, rare classes, boundary conditions:

## Business Validation
- Does the model output make intuitive business sense?
- Have domain experts reviewed sample predictions?
- Sanity check: do feature importances align with domain knowledge?

## Reproducibility
- Code location: `assets/`
- Environment / dependencies:
- Random seed:
- Data snapshot / version:

---
{Insert INVESTIGATE checklist}
```

**04_voice.md for Modeling:**
```markdown
# VOICE: {title}
> ID: {ID} | Type: ðŸ“ˆ Modeling | Stage: ðŸ“¢ VOICE | Updated: {YYYY-MM-DD}

## Executive Summary
(1-3 sentences: what the model does, how well it works, business impact)

## So What â†’ Now What

### Model Performance
- Key metric: {metric} = {value} (target was {target})
- Comparison to baseline: {improvement}
- **So What?** What does this performance level mean in business terms?
- **Now What?** Deploy / iterate / abandon?

### Business Interpretation
- What does the model tell us about the business?
- Top drivers / features â€” do they match domain intuition?
- Unexpected findings:

## Deployment Recommendation
- Deploy to production? (Yes / No / Conditional)
- **Confidence**: ðŸŸ¢ High / ðŸŸ¡ Medium / ðŸ”´ Low
- Expected business impact:
- Rollout strategy: (shadow mode â†’ A/B test â†’ full deployment)

## Trade-off Analysis
- Accuracy vs interpretability:
- Speed vs performance:
- Coverage vs precision:
- Guardrail metrics impact (reference config.md):

## Monitoring Plan
- Key metrics to track in production:
- Model drift detection strategy:
- Retraining trigger conditions:
- Fallback plan if model degrades:

## Limitations & Risks
(First-class content, not a footnote)
- Known failure modes and segments:
- Data freshness requirements:
- What would invalidate this model:
- Fairness concerns:

## Audience-specific Messages

### For {stakeholder}
-

---
{Insert VOICE checklist}
```

**05_evolve.md** uses the same template for both types.

---

#### 4C. Quick Analysis (any type)

Create file: `analyses/active/quick_{ID}_{title-slug}.md`

```markdown
# Quick: {title}
> ID: {ID} | Type: {ðŸ” Investigation / ðŸ“ˆ Modeling} | Status: ðŸŸ¡ In Progress | Started: {YYYY-MM-DD}

## ASK
- Question / Objective:
- Framing: Causation / Correlation
- Top hypotheses: (1) (2) (3)
- Deadline:

## LOOK
- Data source:
- Key segments checked:
- External factors considered:
- Notable findings:

## INVESTIGATE
- Method:
- Hypotheses tested: âœ… / âŒ / âš ï¸
- Result:
- Confidence: ðŸŸ¢ High / ðŸŸ¡ Medium / ðŸ”´ Low

## VOICE
- So What? (business impact):
- Now What? (recommended action):
- Audience:

## EVOLVE
- What would change this conclusion?
- Follow-up needed: Yes / No
- Next question:

---
Check: ðŸŸ¢ Proceed / ðŸ”´ Stop
- [ ] Is the purpose clear and framed (causal/correlational)?
- [ ] Was the data segmented (not just aggregated)?
- [ ] Were alternative hypotheses considered?
- [ ] Does the conclusion answer the question with confidence level?

---
> ðŸ’¡ If this analysis is getting bigger: `/analysis new --from {ID}`
```

---

### Step 5: Update status.md

Add new entry to the Active table in `.analysis/status.md`.
Include the Type column.
Update the "Last updated" timestamp.

### Step 6: Confirmation

Tell the user:
- New analysis created with ID and type
- Show file path(s)
- Reference relevant metrics from config.md if applicable
- For Full: suggest filling out 01_ask.md, then running `/analysis next`
- For Quick: suggest filling out each section in order
- For Modeling: mention "INVESTIGATE will include model comparison and validation templates"
