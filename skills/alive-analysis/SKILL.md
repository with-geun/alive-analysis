# alive-analysis Skill

> Data analysis workflow kit based on the ALIVE loop.
> Provides structured analysis methodology for data analysts and non-analyst roles.

---

## Overview

alive-analysis helps structure data analysis work using the **ALIVE loop**:
**Ask â†’ Look â†’ Investigate â†’ Voice â†’ Evolve**

It serves two personas:
- **Data analysts**: Deep, systematic analysis with full ALIVE flow
- **Non-analyst roles** (PM, engineers, marketers): Quick analysis with guided framework

---

## ALIVE Loop Reference

### Stage 1: ASK (â“)
**Core question**: What do we want to know â€” and WHY?

Purpose:
- Define the problem clearly and confirm the requester's REAL goal (not just what they said)
- Frame the question: Is this about **causation** ("Why did X happen?") or **correlation** ("Are X and Y related?")?
- Set success criteria and scope boundaries
- Build a **hypothesis tree** before touching any data
- Set up **multi-lens perspective**: macro (market/industry) â†’ meso (company/product) â†’ micro (user/session)

#### Hypothesis Tree
Before diving into data, structure thinking:
```
Main question: "Why did D30 retention drop?"
â”œâ”€â”€ Internal factors
â”‚   â”œâ”€â”€ Product changes (releases, feature removals)
â”‚   â”œâ”€â”€ Channel mix changes (acquisition source shift)
â”‚   â”œâ”€â”€ Cross-service impact (did another service change affect this?)
â”‚   â””â”€â”€ Pricing / promotion changes
â”œâ”€â”€ External factors
â”‚   â”œâ”€â”€ Seasonality / holidays
â”‚   â”œâ”€â”€ Competitor actions
â”‚   â”œâ”€â”€ Market / economic shifts
â”‚   â””â”€â”€ Platform changes (iOS/Android policy, algorithm updates)
â””â”€â”€ Data artifacts
    â”œâ”€â”€ Tracking changes (instrumentation broke?)
    â”œâ”€â”€ Definition changes (metric recalculated?)
    â””â”€â”€ Population changes (new user mix shifted?)
```

#### Causal vs Correlational Framing
Ask explicitly:
- "Are we trying to prove X **caused** Y? Or just that they move together?"
- "If we find a correlation, what would we need to prove causation?"
- This determines the methodology: correlation â†’ observational analysis; causation â†’ quasi-experimental or controlled experiment

#### Key Questions to Ask the User
- What triggered this question? (event, dashboard alert, stakeholder request)
- What decision will this analysis inform?
- What's the cost of being wrong?
- Are there related analyses or prior findings to build on?
- What data can you access? (MCP, exported files, BI dashboards)

Common mistakes to prevent:
- Starting analysis without a clear question
- Scope creep â€” trying to answer everything at once
- Not confirming the requester's actual goal (vs stated goal)
- Confusing "interesting" with "actionable"
- Ignoring the hypothesis tree â€” jumping to the first plausible explanation

### Stage 2: LOOK (ğŸ‘€)
**Core question**: What does the data ACTUALLY show â€” and what's missing?

Purpose:
- Review data quality (missing values, outliers, date ranges)
- **Segment before averaging** â€” never trust aggregate numbers alone
- Identify **confounding variables** that could mislead the analysis
- Check for **external factors** (seasonality, holidays, competitor actions)
- Map **cross-service dependencies** â€” changes in service A can affect metrics in service B
- Validate data access methods and establish a query/file pipeline

#### Segmentation Strategy
Always break the data down BEFORE forming conclusions:
- By time: daily/weekly trends, before/after specific events
- By cohort: new vs returning users, acquisition channel, geography
- By platform: iOS vs Android, web vs app
- By segment: free vs paid, high-value vs low-value

Ask: "Does the pattern hold across ALL segments, or is it driven by one?"

#### Confounding Variables
Before attributing any pattern, check:
- Did something else change at the same time? (release, campaign, holiday)
- Is there a **selection bias**? (are we comparing different populations?)
- Is there a **survivorship bias**? (are we only seeing users who stayed?)
- Could this be a **Simpson's paradox**? (aggregate trend vs segment trends)

#### External Data Consideration
- Check the calendar: holidays, industry events, competitor launches
- Check macro context: economic indicators, platform policy changes
- Check company context: other team's releases, cross-service changes
- Reference config.md guardrail metrics â€” are any of them moving too?

#### Data Access During Conversation
- **MCP connected**: AI can run queries directly â€” ask before executing
- **User provides files**: Read CSV/Excel/JSON files provided during conversation
- **BI tool screenshots**: User shares dashboard images â€” AI interprets visually
- **No direct access**: AI generates SQL/Python for user to run, then discusses results

Common mistakes to prevent:
- Looking at averages without segmentation
- Ignoring seasonality and external factors
- Re-verifying already confirmed data
- Skipping data quality checks
- Assuming correlation found in LOOK implies causation

### Stage 3: INVESTIGATE (ğŸ”)
**Core question**: Why is it REALLY happening â€” can we prove it?

Purpose:
- **Eliminate hypotheses** systematically (not just confirm the first one)
- Apply **multi-lens analysis**: macro â†’ meso â†’ micro
- Test for **causation vs correlation** rigorously
- Check **cross-service impacts** and interaction effects
- Perform **sensitivity analysis** â€” how robust are the findings?
- Handle data mid-conversation (MCP queries, file uploads, ad-hoc analysis)

#### Hypothesis Elimination Process
Work through the hypothesis tree from ASK:
1. List all hypotheses (from the tree)
2. For each: What evidence would **support** it? What would **disprove** it?
3. Test the easiest-to-disprove hypotheses first (efficient elimination)
4. Track: âœ… Supported / âŒ Eliminated / âš ï¸ Inconclusive
5. For surviving hypotheses: estimate relative contribution (e.g., "Channel mix explains ~70%, product change ~20%, unknown ~10%")

#### Multi-Lens Analysis Framework
```
Macro (market/industry level)
â”œâ”€â”€ Market trends â€” is this industry-wide?
â”œâ”€â”€ Competitor analysis â€” did competitors change?
â”œâ”€â”€ Economic factors â€” recession, regulation, etc.
â””â”€â”€ Platform/ecosystem â€” iOS policy, algorithm changes?

Meso (company/product level)
â”œâ”€â”€ Cross-service impact â€” did another team's change affect us?
â”œâ”€â”€ Channel mix â€” acquisition source distribution shift?
â”œâ”€â”€ Product changes â€” releases, A/B tests, feature flags?
â””â”€â”€ Business operations â€” pricing, campaigns, partnerships?

Micro (user/session level)
â”œâ”€â”€ User behavior patterns â€” funnel analysis, session depth
â”œâ”€â”€ Cohort-specific trends â€” new vs returning, by segment
â”œâ”€â”€ Edge cases â€” specific user groups, extreme behaviors
â””â”€â”€ Qualitative signals â€” CS tickets, reviews, survey data
```

#### Causation Testing (when causal claims are needed)
- **Time ordering**: Did the cause precede the effect?
- **Mechanism**: Is there a plausible pathway from cause to effect?
- **Dose-response**: Does more of the cause produce more of the effect?
- **Counterfactual**: What happened to the control group / unaffected segment?
- **Consistency**: Does the pattern hold across different segments and time periods?
- If true experiment isn't possible â†’ quasi-experimental methods (diff-in-diff, regression discontinuity, propensity score matching)

#### Cross-Service Impact Analysis
In organizations with multiple products/services:
- Map service dependencies: "If service A changes onboarding, does service B's retention change?"
- Check shared resources: same user base, shared auth, shared data pipeline
- Look for cannibalisation: did a new feature pull users from an existing one?
- Check infrastructure: shared API, CDN, DB performance impacts

#### Sensitivity Analysis
Before finalizing findings, test robustness:
- "If we change the date range by Â±1 week, does the conclusion hold?"
- "If we exclude the top/bottom 5% of users, does the pattern persist?"
- "If we use a different metric definition, do we get the same result?"
- "What's the minimum effect size that would be actionable?"

#### Mid-Conversation Data Handling
- When user provides a file: read it, summarize structure, ask what to look for
- When MCP is available: propose queries, get confirmation, run and discuss results
- When user shares screenshots: interpret patterns, ask clarifying questions
- When running ad-hoc analysis: document every query/step for reproducibility in `assets/`

Common mistakes to prevent:
- Confirmation bias â€” only looking for supporting evidence
- Stopping at the first plausible explanation without testing alternatives
- Claiming causation from correlation alone
- Ignoring external and cross-service factors
- Not recording queries/code for reproduction
- Over-complicating analysis when a simple comparison suffices

### Stage 4: VOICE (ğŸ“¢)
**Core question**: So what â€” and now what?

Purpose:
- Apply the **"So What â†’ Now What"** framework for every finding
- Assign **confidence levels** to each conclusion
- Frame recommendations with **trade-off analysis**
- Tailor messages to different audiences using config.md stakeholder list
- Make limitations a **first-class part** of the story, not a footnote

#### "So What â†’ Now What" Framework
For every finding, answer both questions:
```
Finding: "D30 retention dropped 8pp, driven by TikTok-acquired users"
â”œâ”€â”€ So What?  â†’ "TikTok users have 3x lower retention than organic.
â”‚                Our channel mix shifted from 20% to 45% TikTok."
â””â”€â”€ Now What? â†’ "Option A: Reduce TikTok budget, reallocate to higher-LTV channels
                 Option B: Keep TikTok but improve onboarding for these users
                 Option C: Accept lower D30 if TikTok CAC justifies it on LTV basis"
```

#### Confidence Levels
Tag each finding explicitly:
- ğŸŸ¢ **High confidence**: Multiple data sources confirm, robust to sensitivity checks, clear mechanism
- ğŸŸ¡ **Medium confidence**: Supported by data but with caveats (small sample, single source, confounders possible)
- ğŸ”´ **Low confidence**: Suggestive only â€” needs further analysis, could be noise or artifact

Include reasoning: "High confidence because the pattern holds across 3 months, 2 platforms, and survives exclusion of outliers."

#### Trade-off Analysis
For each recommendation, make the trade-offs explicit:
- What do we **gain** if we act on this?
- What do we **risk** or lose?
- What's the **cost of inaction**?
- Reference guardrail metrics from config.md: "This recommendation would improve conversion but check impact on refund rate."

#### Audience-Specific Communication
- **Executives / C-level**: Lead with business impact in one sentence. Numbers, not methodology. "So what does this mean for revenue?"
- **Product / Engineering**: Include technical detail. "Which feature, which release, which segment." Actionable next steps.
- **Cross-functional (marketing, ops)**: Connect to their KPIs. "Here's how this affects your campaign ROI."
- Reference stakeholders from config.md to auto-suggest audience sections.

Common mistakes to prevent:
- Burying the lead â€” not stating the conclusion first
- Presenting findings without "So what?" and "Now what?"
- Overstating confidence â€” not flagging uncertainty
- Using jargon with non-technical audiences
- Not answering the original question from ASK
- Presenting trade-offs as one-sided recommendations

### Stage 5: EVOLVE (ğŸŒ±)
**Core question**: What would change our conclusion â€” and what should we ask next?

Purpose:
- **Robustness check**: Explicitly ask "What would change our conclusion?"
- Identify unanswered questions and propose follow-up analyses
- Set up **monitoring**: if we're right, what should we watch going forward?
- Capture **reusable knowledge** for future analyses
- Connect back to the **North Star metric** from config.md

#### Conclusion Robustness Check
Before finalizing, stress-test the analysis:
- "What new data could **disprove** our conclusion?"
- "What **assumptions** did we make that we didn't verify?"
- "If a colleague challenged this, what would they attack first?"
- "In 3 months, what would make us say 'we were wrong'?"

#### Monitoring Setup
If the analysis identified a real issue or opportunity:
- What metric should be tracked going forward?
- What threshold should trigger a re-investigation?
- Can we set up a dashboard or alert? (reference data stack from config.md)
- Who should own the monitoring?

#### Knowledge Capture
- What reusable patterns emerged? (SQL templates, segmentation approaches, data gotchas)
- What did we learn about the data that future analyses should know?
- Are there commonly-used queries worth saving to `assets/`?
- Did we discover any data quality issues to flag to the data engineering team?

#### Connect to North Star
- Reference the North Star metric from config.md: "How does this analysis connect to {metric}?"
- "Does this change our understanding of what drives {North Star}?"
- "Should our metric framework be updated based on these findings?"

Common mistakes to prevent:
- Treating the analysis as "done" without reflection
- Not capturing follow-up ideas while they're fresh
- Forgetting to set up monitoring for identified issues
- Missing the connection between this analysis and the bigger picture

---

## Checklist Templates

### Default: ASK Checklist
```markdown
## Checklist â€” ASK
- [ ] ğŸŸ¢/ğŸ”´ Have you accurately identified the requester's REAL goal (not just stated goal)?
- [ ] ğŸŸ¢/ğŸ”´ Is the question framed as causal or correlational?
- [ ] ğŸŸ¢/ğŸ”´ Have you built a hypothesis tree (not just one guess)?
- [ ] ğŸŸ¢/ğŸ”´ Have you secured relevant domain knowledge?
- [ ] ğŸŸ¢/ğŸ”´ Have you created an analysis plan that fits the timeline?
- [ ] ğŸŸ¢/ğŸ”´ Have you confirmed the data specification and access method?
- [ ] ğŸŸ¢/ğŸ”´ Have you considered appropriate sample size?
```

### Default: LOOK Checklist
```markdown
## Checklist â€” LOOK
- [ ] ğŸŸ¢/ğŸ”´ Have you segmented the data before drawing conclusions?
- [ ] ğŸŸ¢/ğŸ”´ Have you checked for confounding variables?
- [ ] ğŸŸ¢/ğŸ”´ Have you considered external factors (seasonality, competitors, market)?
- [ ] ğŸŸ¢/ğŸ”´ Have you checked for cross-service impacts?
- [ ] ğŸŸ¢/ğŸ”´ Is the sampling method appropriate?
- [ ] ğŸŸ¢/ğŸ”´ Have you checked for data errors (outliers, missing values)?
- [ ] ğŸŸ¢/ğŸ”´ Are you only performing analysis needed for the problem?
```

### Default: INVESTIGATE Checklist
```markdown
## Checklist â€” INVESTIGATE
- [ ] ğŸŸ¢/ğŸ”´ Have you tested MULTIPLE hypotheses (not just confirmed one)?
- [ ] ğŸŸ¢/ğŸ”´ Have you applied multi-lens analysis (macro/meso/micro)?
- [ ] ğŸŸ¢/ğŸ”´ If claiming causation, have you verified: time ordering, mechanism, counterfactual?
- [ ] ğŸŸ¢/ğŸ”´ Have you performed sensitivity analysis (robustness check)?
- [ ] ğŸŸ¢/ğŸ”´ Have you clearly handled outliers/anomalies?
- [ ] ğŸŸ¢/ğŸ”´ Have you assigned confidence levels to each finding?
- [ ] ğŸŸ¢/ğŸ”´ Can the results be reproduced? (queries/code recorded in assets/)
```

### Default: VOICE Checklist
```markdown
## Checklist â€” VOICE
- [ ] ğŸŸ¢/ğŸ”´ Have you applied "So What â†’ Now What" for each finding?
- [ ] ğŸŸ¢/ğŸ”´ Have you tagged confidence levels (ğŸŸ¢/ğŸŸ¡/ğŸ”´) with reasoning?
- [ ] ğŸŸ¢/ğŸ”´ Have you included trade-off analysis for recommendations?
- [ ] ğŸŸ¢/ğŸ”´ Have you checked guardrail metrics impact?
- [ ] ğŸŸ¢/ğŸ”´ Are limitations visible (not buried in a footnote)?
- [ ] ğŸŸ¢/ğŸ”´ Have you tailored messages for each stakeholder audience?
```

### Default: EVOLVE Checklist
```markdown
## Checklist â€” EVOLVE
- [ ] ğŸŸ¢/ğŸ”´ Have you stress-tested the conclusion (what would disprove it)?
- [ ] ğŸŸ¢/ğŸ”´ Have you set up monitoring for identified issues?
- [ ] ğŸŸ¢/ğŸ”´ Are follow-up questions specifically defined?
- [ ] ğŸŸ¢/ğŸ”´ Have you captured reusable knowledge for future analyses?
- [ ] ğŸŸ¢/ğŸ”´ Have you connected findings back to the North Star metric?
- [ ] ğŸŸ¢/ğŸ”´ Have you summarized the key insight in one sentence?
```

---

## Quick Analysis Checklist (Abbreviated)

For Quick mode, use these 4 items:
```markdown
Check: ğŸŸ¢ Proceed / ğŸ”´ Stop
- [ ] Is the purpose clear and framed (causal/correlational)?
- [ ] Was the data segmented (not just aggregated)?
- [ ] Were alternative hypotheses considered?
- [ ] Does the conclusion answer the question with confidence level?
```

---

## ID Format

- **Full**: `F-{YYYY}-{MMDD}-{sequence}` (e.g., `F-2026-0210-001`)
- **Quick**: `Q-{YYYY}-{MMDD}-{sequence}` (e.g., `Q-2026-0210-002`)
- Sequence resets daily, starts at 001

---

## Stage Icons

```
â“ ASK â†’ ğŸ‘€ LOOK â†’ ğŸ” INVESTIGATE â†’ ğŸ“¢ VOICE â†’ ğŸŒ± EVOLVE
âœ… Archived | â³ Pending | ğŸŸ¡ In Progress
```

---

## File Naming Conventions

- Full analysis folder: `{ID}_{title-slug}/` (e.g., `F-2026-0210-001_dau-drop-investigation/`)
- Quick analysis file: `quick_{ID}_{title-slug}.md`
- Title slug: lowercase, hyphens, no special characters
- Stage files: `01_ask.md`, `02_look.md`, `03_investigate.md`, `04_voice.md`, `05_evolve.md`

---

## Archive Rules

1. Archive after VOICE + EVOLVE are complete
2. Generate `summary.md` with key insight, findings, and reproduction info
3. Move from `analyses/active/` to `analyses/archive/{YYYY-MM}/`
4. Update status.md (remove from Active, add to Recently Archived)
5. Keep max 5 entries in Recently Archived

---

## Language Support

- Document language is set in `.analysis/config.md`
- Checklists, templates, and status messages follow the configured language
- Default: Korean (í•œêµ­ì–´)
- Supported: Korean, English, Japanese

---

## Interaction Guidelines

**CRITICAL: alive-analysis is a conversational workflow.**
Do NOT auto-fill analysis content. Each stage must be a dialogue with the user.
The AI is a guide and co-analyst, not an auto-generator.

### Core Principles

1. **Ask, don't assume** â€” Every stage starts with questions to the user.
2. **One stage at a time** â€” Never skip ahead. Complete the current stage through conversation before moving on.
3. **User writes the insight, AI structures it** â€” The user provides domain knowledge and judgment. AI helps organize, challenge, and document.
4. **Pause at checklists** â€” Before advancing, walk through the checklist WITH the user. Each item is a conversation, not a checkbox to auto-fill.

### Stage-by-Stage Conversation Flow

#### ASK Stage â€” Frame the question before touching data
```
AI: "What question are you trying to answer?"
User: (describes the problem)
AI: "Got it. Before we dive in â€” is this about WHY something happened (causation),
     or whether two things are related (correlation)?"
User: (clarifies intent)
AI: "Who requested this? What decision will this inform?"
User: (provides context)
AI: "Let me build a hypothesis tree. Here are possible explanations:"
    - Internal: product changes, channel mix, cross-service effects
    - External: seasonality, competitors, market shifts
    - Data artifacts: tracking changes, definition changes
AI: "Which of these feel most likely? Any I should add?"
User: (prioritizes hypotheses)
AI: "What's in scope and out of scope? And the deadline?"
User: (confirms)
AI: â†’ Writes 01_ask.md with hypothesis tree and framing
AI: "Let's check before moving on:"
    - "Is the question framed as causal or correlational? ğŸŸ¢/ğŸ”´"
    - "Do we have a hypothesis tree, not just one guess? ğŸŸ¢/ğŸ”´"
    - "Do we know where the data lives? ğŸŸ¢/ğŸ”´"
AI: "Ready to move to LOOK? Or anything to add?"
```

#### LOOK Stage â€” Segment, don't just aggregate
```
AI: "What data sources will we use? Do you have MCP access, or will you
     run queries and share results?"
User: (describes access)
[If MCP]: AI proposes query â†’ gets confirmation â†’ runs â†’ discusses results
[If file]: User shares file â†’ AI reads â†’ summarizes structure â†’ asks what to look for
[If manual]: AI generates SQL/Python â†’ user runs â†’ shares results

AI: "Let's not look at averages yet. How should we segment this?"
    - "By time period (before/after event)?"
    - "By user cohort (new vs returning, by channel)?"
    - "By platform (iOS/Android, web/app)?"
User: (chooses segmentation)
AI: "Any external factors to check? I see {date} is near a holiday /
     your competitor launched X / there was a platform update..."
User: (confirms or adds context)
AI: "Let me also check â€” did anything change in adjacent services?
     Sometimes changes in service A affect metrics in service B."
User: (discusses cross-service)
AI: "Before we move on, let me check confounders:
    - Did anything else change at the same time?
    - Are we comparing apples to apples (same population)?
    - Could this be survivorship bias?"
User: (validates)
AI: â†’ Updates 02_look.md with segmented findings
AI: "Checklist review:"
    - "Did we segment before drawing conclusions? ğŸŸ¢/ğŸ”´"
    - "Have we checked for confounders and external factors? ğŸŸ¢/ğŸ”´"
    - "Is the data quality confirmed? ğŸŸ¢/ğŸ”´"
```

#### INVESTIGATE Stage â€” Eliminate, don't just confirm
```
AI: "From our hypothesis tree in ASK, let's work through them systematically.
     Which is easiest to disprove? Let's start there."
User: (picks hypothesis)
AI: "What evidence would DISPROVE this hypothesis? Let's look for that first."
User: (discusses disproving evidence)
[AI runs queries via MCP or asks user to check]

AI: "Let's look at this from multiple angles:"
    - "Macro: Is this happening industry-wide, or just us?"
    - "Meso: Is this specific to one product/channel, or company-wide?"
    - "Micro: What does user-level behavior look like?"
User: (discusses each level)
AI: "Based on what we've found so far, here's the scorecard:
     âœ… Hypothesis 1 (channel mix): Strong evidence â€” explains ~70%
     âŒ Hypothesis 2 (bug): Disproved â€” no change in error rates
     âš ï¸ Hypothesis 3 (competitor): Inconclusive â€” need more data"
User: (validates or challenges)

[If causal claim needed]:
AI: "You want to say X CAUSED Y. Let's check:
    - Time ordering: Did X happen before Y? âœ…
    - Mechanism: Is there a plausible pathway?
    - Dose-response: More X â†’ more Y?
    - Counterfactual: Unaffected segment stayed stable?"
User: (works through each)

AI: "Sensitivity check â€” does this hold if we:
    - Change the date range by a week?
    - Exclude outliers?
    - Use a different metric definition?"
User: (confirms robustness)
AI: â†’ Updates 03_investigate.md with hypothesis results and evidence
AI: "Can someone else reproduce this? Let's save queries to assets/."
```

#### VOICE Stage â€” So What, Now What
```
AI: "Let's frame each finding with 'So What â†’ Now What'."
AI: "Finding 1: {summary}. So what does this mean for the business?"
User: (interprets business impact)
AI: "Now what? What are the options?"
User: (proposes actions)
AI: "Let me add trade-offs for each option:
    - Option A: {benefit} but {risk}
    - Option B: {benefit} but {risk}
    Does this affect any guardrail metrics from your config?"
User: (evaluates)

AI: "Confidence level on each finding:"
    - Finding 1: ğŸŸ¢ High â€” multiple sources, robust to sensitivity checks
    - Finding 2: ğŸŸ¡ Medium â€” supported but small sample
AI: "Does that feel right?"
User: (adjusts)

AI: "Who needs to hear this? From your config, stakeholders are: {list}"
    - "For {executive}: What's the one-sentence version?"
    - "For {product team}: What's the technical detail they need?"
    - "For {marketing}: How does this affect their metrics?"
User: (tailors messages)
AI: â†’ Updates 04_voice.md
AI: "Important â€” what are the limitations and caveats? Let's make those visible,
     not buried in a footnote."
```

#### EVOLVE Stage â€” Stress-test and look forward
```
AI: "Before we close â€” let's stress-test our conclusion."
    - "What new data could DISPROVE what we found?"
    - "What assumptions did we make that we didn't verify?"
    - "If a colleague challenged this, what would they attack?"
User: (identifies vulnerabilities)

AI: "Should we set up monitoring for this?"
    - "What metric should we track going forward?"
    - "What threshold should trigger re-investigation?"
    - "Who should own it?"
User: (sets up monitoring plan)

AI: "What questions came up that we didn't answer?"
User: (lists open questions)
AI: "Any reusable patterns from this analysis? SQL templates, segmentation
     approaches, data gotchas that future analyses should know about?"
User: (captures learnings)

AI: "Connecting back to your North Star ({metric from config}):
     Does this change our understanding of what drives it?"
User: (reflects)
AI: "One sentence â€” the single most important takeaway?"
User: (provides insight)
AI: â†’ Updates 05_evolve.md
AI: "Ready to archive? Or want to revisit any stage?"
```

### Quick Mode Conversation

Quick mode follows the same methodology but compressed. The AI still guides through each lens but faster:
```
AI: "What's the question? Is this a 'why did X happen' or 'are X and Y related'? (ASK)"
User: "Why did signups drop yesterday?"
AI: "Quick hypothesis: internal (bug, release, tracking), external (competitor, platform)?
     Which feels most likely?"
User: "Probably internal â€” we had a release yesterday"

AI: "Where will you look? Let's segment: which platform, which user type? (LOOK)"
User: "GA4 events and server logs â€” mobile seems affected"
AI: "Any external factors? Holiday? Competitor launch?"
User: "No, nothing I know of"

AI: "What did you find? (INVESTIGATE)"
User: "Mobile auth API was down 14-17h"
AI: "Can we confirm: was web unaffected? Does timing match the drop exactly?"
User: "Yes, web was fine. Drop is exactly 14-17h."

AI: "So what â€” and now what? (VOICE)"
User: "API maintenance caused mobile signup failures. We need a fallback."
AI: "Confidence? ğŸŸ¢ High â€” timing matches, web unaffected, clear mechanism."

AI: "Any follow-up? Should we monitor this? (EVOLVE)"
User: "Add email auth as fallback. Set up uptime alert for auth API."
AI: â†’ Writes the complete Quick file with all sections
AI: "Checklist: purpose clear? âœ… data segmented? âœ… conclusion answers question? âœ…
     hypothesis tested (not just assumed)? âœ…"
AI: "Archive this, or keep it active?"
```

### Modeling Mode Conversation Adjustments

For ğŸ“ˆ Modeling analyses, the conversation focus shifts at each stage:
- **ASK**: "What are you trying to predict/classify?" + business impact + success criteria (AUC, MAPE targets)
- **LOOK**: Target variable distribution, feature exploration, leakage risk, train/val/test split strategy
- **INVESTIGATE**: Baseline â†’ model comparison â†’ best model analysis â†’ error analysis â†’ reproducibility
- **VOICE**: Model performance vs target, business interpretation, deploy recommendation, monitoring plan
- **EVOLVE**: Model drift risk, retraining schedule, feature pipeline automation, A/B test proposal

### Mid-Conversation Data Handling

The AI should seamlessly handle data that arrives during the conversation:

1. **User provides a file** (CSV, Excel, JSON, SQL dump):
   - Read the file, summarize: rows, columns, date range, notable patterns
   - Ask: "What should I look for in this data?"
   - Document the file in the analysis's `assets/` folder

2. **MCP is connected** (database access):
   - Propose a query in natural language first
   - Show the actual SQL/query and get confirmation before running
   - Discuss results, then propose next query based on findings
   - Save all queries to `assets/` for reproducibility

3. **User shares a screenshot** (dashboard, chart):
   - Interpret the visual: trends, anomalies, patterns
   - Ask clarifying questions: "Is this Y-axis absolute or percent?"
   - Note the source for documentation

4. **No direct data access**:
   - Generate SQL/Python code for user to execute
   - Ask user to paste results back
   - AI interprets and continues the conversation

### What NOT to do

- âŒ Generate analysis content without asking the user
- âŒ Skip stages or combine multiple stages at once
- âŒ Auto-check all checklist items without discussion
- âŒ Move to the next stage without user confirmation
- âŒ Make assumptions about data, methods, or conclusions
- âŒ Claim causation without testing for it (time ordering, mechanism, counterfactual)
- âŒ Present aggregate numbers without segmentation
- âŒ Ignore external factors and cross-service impacts
- âŒ Stop at the first plausible hypothesis without testing alternatives
- âŒ Present findings without "So What?" and "Now What?"
- âŒ Skip sensitivity analysis â€” always check robustness
- âŒ Run MCP queries or read files without user confirmation
