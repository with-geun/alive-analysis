# /experiment next

Advance the current experiment to the next stage.

## Instructions

### Step 1: Identify current experiment

Read `.analysis/status.md` to find active experiments (Type: ğŸ§ª Experiment).

- If only 1 active Full experiment â†’ select it automatically
- If multiple active â†’ ask user which experiment to advance (show ID + title)
- Quick experiments don't use `/experiment next` (all sections are in one file)
  - If user selects a Quick, remind them to fill sections in order within the file

### Step 2: Determine current stage

Read the experiment folder to see which stage files exist:
- Only `01_design.md` â†’ current stage is DESIGN
- Up to `02_validate.md` â†’ current stage is VALIDATE
- Up to `03_analyze.md` â†’ current stage is ANALYZE
- Up to `04_decide.md` â†’ current stage is DECIDE
- All 5 files exist â†’ current stage is LEARN (experiment is complete)

### Step 3: Review current stage checklist

Read the current stage's checklist (embedded at the bottom of the current stage file).

Check if there are any ğŸ”´ (stop) items:
- If ğŸ”´ items exist â†’ warn the user: "There are stop signals in your {stage} checklist. Review before proceeding."
- If all items are unchecked â†’ remind: "Don't forget to review the checklist before moving on."
- If items are checked â†’ proceed

**Special gate: DESIGN â†’ VALIDATE**
Before advancing from DESIGN, verify:
- Pre-registration section is filled (this MUST be locked before seeing any data)
- If empty, warn: "Your analysis plan should be pre-registered before starting the experiment. Fill the Pre-registration section in 01_design.md first."

### Step 4: Generate next stage file

Based on current stage, generate the next file.

**DESIGN â†’ VALIDATE** (generate `02_validate.md`):
```markdown
# VALIDATE: {title}
> ID: {ID} | Type: ğŸ§ª Experiment | Stage: âœ… VALIDATE | Updated: {YYYY-MM-DD}

## Pre-Experiment Checks

### AA Test / Historical Balance
- Method: (AA test on pre-period data / historical segment comparison)
- Result: Groups are balanced / not balanced
- Key metrics checked:
  | Metric | Control | Treatment | Î” | Balanced? |
  |--------|---------|-----------|---|-----------|
  | | | | | |

### Sample Ratio Mismatch (SRM)
- Expected ratio: {from 01_design.md}
- Actual ratio: {observed}
- Chi-square test p-value: (p < 0.001 indicates SRM)
- SRM detected? No / Yes â†’ âš ï¸ investigate before proceeding

> ğŸ’¡ SRM indicates a problem with randomization. Common causes:
> - Bot filtering differences between variants
> - Redirect timing differences
> - Initialization bias (treatment takes longer to load)
> If SRM is detected, DO NOT proceed to Analyze. Fix the root cause first.

### Segment Balance
| Segment | Control % | Treatment % | Balanced? |
|---------|-----------|-------------|-----------|
| New users | | | |
| Returning users | | | |
| Platform (iOS/Android/Web) | | | |
| {custom segment} | | | |

### Instrumentation Check
- [ ] Primary metric events logging correctly
- [ ] Secondary metric events logging correctly
- [ ] Guardrail metric events logging correctly
- [ ] No duplicate counting
- [ ] Attribution window correctly set

## External Factors
- Holidays/events during experiment period:
- Planned releases/deployments:
- Marketing campaigns:
- Other active experiments on same population:

## Ramp-Up Log
| Date | Traffic % | Observation | Action |
|------|-----------|-------------|--------|
| {start} | {initial}% | | |

## Validation Summary
- Ready to proceed? âœ… Yes / ğŸ”´ No â€” {reason}
- Issues found:
- Mitigation:

---
{Insert VALIDATE checklist from ab-tests/checklists/validate.md}
```

**VALIDATE â†’ ANALYZE** (generate `03_analyze.md`):

**Important**: Before generating this file, ask the user: "Has the experiment reached the minimum duration from 01_design.md? Ending early risks false conclusions."

If the experiment was stopped before the minimum duration, add a warning banner at the top of the file:
```
> âš ï¸ **EARLY STOPPING WARNING**: This experiment ran {X} of {Y} planned days.
> Results may be inflated due to optional stopping bias. Post-launch monitoring is critical.
```

```markdown
# ANALYZE: {title}
> ID: {ID} | Type: ğŸ§ª Experiment | Stage: ğŸ”¬ ANALYZE | Updated: {YYYY-MM-DD}

## Experiment Summary
- Duration: {start} to {end} ({N} days)
- Sample size: Control = {nâ‚}, Treatment = {nâ‚‚}
- Total: {N}

## SRM Re-check (Final)
- Expected ratio: {from design}
- Final ratio: Control {nâ‚} / Treatment {nâ‚‚} = {ratio}
- SRM test p-value:
- Result: âœ… No SRM / ğŸ”´ SRM detected â†’ results may be invalid

## Primary Metric Results
| | Control | Treatment | Î” (absolute) | Î” (relative) | 95% CI | p-value |
|--|---------|-----------|---------------|--------------|--------|---------|
| {metric} | | | | | | |

- **Statistically significant?** Yes (p < 0.05) / No
- **Practically significant?** Is the effect size meaningful for the business?
  - MDE was: {from design}
  - Observed effect: {actual}
  - "Even if significant, is {Î”} worth the cost of shipping this?"

> ğŸ’¡ Statistical significance â‰  practical significance.
> A tiny effect can be "significant" with large samples.
> Always ask: "Would I make a different decision knowing this effect size?"

## Secondary Metrics
| Metric | Control | Treatment | Î” | p-value | Direction | Notable? |
|--------|---------|-----------|---|---------|-----------|----------|
| | | | | | | |
| | | | | | | |

## Guardrail Metrics
| Metric | Control | Treatment | Î” | Acceptable? | Threshold |
|--------|---------|-----------|---|-------------|-----------|
| | | | | âœ… / ğŸ”´ | from config.md |

**Guardrail verdict**: All clear / âš ï¸ {metric} degraded

## Statistical Method
- Test used: {t-test / chi-square / Mann-Whitney / z-test for proportions}
- Why this test: {justification based on metric type and distribution}
- One-sided or two-sided:
- Multiple comparison correction: {none / Bonferroni / Holm / FDR} (if >2 variants)
- Bayesian analysis (optional):
  - P(treatment > control):
  - Expected loss:
  - Credible interval:

## Segment Analysis
> Check if the overall effect holds across segments â€” watch for Simpson's Paradox.

| Segment | n (C) | n (T) | Î” | p-value | Different from overall? |
|---------|-------|-------|---|---------|------------------------|
| New users | | | | | |
| Returning users | | | | | |
| {platform} | | | | | |
| {custom} | | | | | |

- Simpson's Paradox risk: {overall effect driven by one segment?}

> ğŸ’¡ Segment analysis note: Report all pre-planned segments from the pre-registration. For exploratory segments discovered during analysis, note them separately â€” use Holm-Bonferroni correction if making segment-level decisions.

## Time Series Analysis
- Effect over time: Stable / Growing / Declining
- Novelty effect risk: {first few days showed large effect that faded?}
- Primacy effect risk: {effect grew over time as users learned?}
- Day-of-week patterns:

## Sensitivity Checks
- Exclude first day (initialization effects): Same result?
- Exclude outliers (top/bottom 1%): Same result?
- Different attribution window: Same result?
- Winsorized metrics (if applicable): Same result?

## Reproducibility
- Query/notebook location: `assets/`
- Steps to reproduce:

---
{Insert ANALYZE checklist from ab-tests/checklists/analyze.md}
```

**ANALYZE â†’ DECIDE** (generate `04_decide.md`):
```markdown
# DECIDE: {title}
> ID: {ID} | Type: ğŸ§ª Experiment | Stage: ğŸ DECIDE | Updated: {YYYY-MM-DD}

## Decision

### Verdict: {ğŸš€ Launch / ğŸ”´ Kill / â³ Extend / ğŸ”„ Iterate}

### Decision Matrix
| Criterion | Result | Pass? |
|-----------|--------|-------|
| Primary metric improved? | {Î”, p-value} | âœ… / ğŸ”´ |
| Effect â‰¥ MDE? | {observed vs MDE} | âœ… / ğŸ”´ |
| Guardrails safe? | {status} | âœ… / ğŸ”´ |
| No segment shows harm? | {segment check} | âœ… / ğŸ”´ |
| Consistent with pre-registration? | {any deviation?} | âœ… / ğŸ”´ |

### Decision Rules (from pre-registration)
| Scenario | Decision |
|----------|----------|
| Primary âœ… + Guardrails âœ… | ğŸš€ Launch |
| Primary ğŸ”´ or Guardrails ğŸ”´ | ğŸ”´ Kill |
| Primary trending âœ… but underpowered | â³ Extend {N more days} |
| Primary mixed (some segments âœ…, some ğŸ”´) | ğŸ”„ Iterate (redesign treatment) |

### Reasoning
- (Why this decision, in 2-3 sentences)

## Impact Estimate
- Expected impact on primary metric:
- Expected impact on North Star metric (from config.md):
- Revenue/cost implication:
- Users affected:

## Rollout Plan (if Launch)
- Rollout strategy: {immediate 100% / gradual ramp / segment-first}
- Ramp schedule:
- Kill switch: {how to revert if issues arise post-launch}
- Monitoring plan post-launch:

## Risk Assessment (if Launch)
- Long-term effects unknown: {list}
- Interaction with upcoming changes: {list}
- Edge cases not covered: {list}
- Optional stopping bias: {if experiment was stopped early, note the risk}

## Salvage Assessment (if Kill or Iterate)
- Is there any segment where the treatment worked? (Could we ship to that segment only?)
- Which components of the treatment are still valuable?
- What modified version might work? (Informs Iterate decision)
- Root cause of failure: (Informs next experiment design)

## Multi-Variant Results (if >2 variants)
| Variant | Primary Î” | p-value (adj) | Guardrails | Verdict |
|---------|-----------|---------------|------------|---------|
| Treatment A | | | | âœ… / ğŸ”´ |
| Treatment B | | | | âœ… / ğŸ”´ |

- Best performing variant:
- Decision: Launch {variant} / Run pairwise follow-up / Kill all

## Audience-Specific Messages

### For Product/PM
-

### For Engineering
-

### For Leadership
-

## Limitations & Caveats
- What this experiment did NOT test:
- Population limitations:
- Duration limitations:

---
{Insert DECIDE checklist from ab-tests/checklists/decide.md}
```

**DECIDE â†’ LEARN** (generate `05_learn.md`):
```markdown
# LEARN: {title}
> ID: {ID} | Type: ğŸ§ª Experiment | Stage: ğŸ“š LEARN | Updated: {YYYY-MM-DD}

## Experiment Outcome Summary
- **Hypothesis**: {from 01_design.md}
- **Result**: Confirmed / Rejected / Inconclusive
- **Decision**: ğŸš€ Launched / ğŸ”´ Killed / â³ Extended / ğŸ”„ Iterated
- **Actual effect**: {observed Î”} on {primary metric}

## What We Learned
- What went as expected?
- What was surprising?
- What would we do differently in the experiment design?

## Post-Launch Monitoring (if launched)
| Checkpoint | Date | Primary Metric | Guardrail | Status | Notes |
|------------|------|----------------|-----------|--------|-------|
| Week 1 | | | | | |
| Week 2 | | | | | |
| Week 4 | | | | | |
| Week 12 | | | | | |

- Long-term effect: Stable / Growing / Declining / Unknown yet
- Any unexpected side effects:

## Hypothesis Evolution
- Original hypothesis correct? Fully / Partially / No
- Updated understanding:
- New hypotheses generated:

## Follow-Up Experiments
- [ ] {experiment idea} â€” to test {what}
- [ ] {experiment idea} â€” to test {what}

## Follow-Up Analyses
- [ ] {analysis} â†’ `/analysis new` (e.g., deeper segment investigation)
- [ ] {analysis} â†’ `/analysis new`

## Proposed New Metrics
> If this experiment revealed a gap in the metric framework, define it here.
> Say "I think we need a metric for {X}" and the AI will guide you through it.

### Metric: {name}

**Background**
- What gap did this experiment reveal in the current metric framework?
- Trigger:
- Replaces or complements:

**Purpose**
- Decision this metric informs:
- Primary audience:
- Proposed tier: ğŸŒŸ North Star / ğŸ“Š Leading / ğŸ›¡ï¸ Guardrail / ğŸ”¬ Diagnostic

**Definition & Logic**
- Formula / calculation:
- Data source:
- Granularity: (daily / weekly / monthly / per-cohort)
- Edge cases handled:

**Interpretation Guide**
- Healthy range:
- Alert threshold:
- Counter-metric:
- Plain-language: "When this goes up, it means... When it drops, it means..."

**STEDII Validation**
- [ ] **Sensitive** â€” Can it detect real changes?
- [ ] **Trustworthy** â€” Is the data accurate and the definition unambiguous?
- [ ] **Efficient** â€” Can it be computed in a practical timeframe?
- [ ] **Debuggable** â€” When it moves, can you decompose WHY?
- [ ] **Interpretable** â€” Does the team understand it without a 5-minute explanation?
- [ ] **Inclusive** â€” Does it fairly represent all user segments?

**Action**
- [ ] Add to `.analysis/config.md` â†’ {tier}
- [ ] Set up dashboard / alert
- [ ] Communicate definition to stakeholders

(Copy this block for additional metrics)

## Knowledge Capture
- Reusable experiment patterns:
- Audience/segment insights:
- Technical gotchas (instrumentation, SRM causes):
- Saved queries/notebooks: `assets/`

## Connection to Analyses
- Related analyses: {list analysis IDs}
- This experiment was triggered by: {analysis ID, if any}
- Next analysis from these results: {planned}

## One-Sentence Takeaway
> (Capture the single most important learning)

---
{Insert LEARN checklist from ab-tests/checklists/learn.md}
```

**LEARN reached** â†’ Tell user:
"This experiment is complete. Run `/experiment archive` to archive it."

### Step 5: Update status.md

Update the experiment row in `.analysis/status.md`:
- Change the stage column to the new stage icon
- Update "Last updated" timestamp

Stage icons for experiments:
```
ğŸ“ DESIGN â†’ âœ… VALIDATE â†’ ğŸ”¬ ANALYZE â†’ ğŸ DECIDE â†’ ğŸ“š LEARN
```

### Step 6: Confirmation

Tell the user:
- Advanced from {old stage} to {new stage}
- Show the new file path
- Remind them to fill in the content and review the checklist
- **DESIGN â†’ VALIDATE**: "Make sure your Pre-registration section is locked. Don't peek at results yet!"
- **VALIDATE â†’ ANALYZE**: "Has the experiment run for the minimum duration? Stopping early risks false conclusions."
- **ANALYZE â†’ DECIDE**: "Use the pre-registered decision criteria. Resist the urge to move the goalposts."
- **DECIDE â†’ LEARN**: "Set up post-launch monitoring checkpoints. The experiment isn't truly done until you've validated the long-term effect."
