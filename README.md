# alive-analysis

**Make every data analysis traceable, repeatable, and team-shareable â€” inside your AI coding agent.**

---

## ğŸš€ TL;DR

Turn AI into a **structured analysis partner** instead of a one-shot answer machine.
Every analysis follows 5 stages (ASK â†’ LOOK â†’ INVESTIGATE â†’ VOICE â†’ EVOLVE), gets saved as Git-tracked markdown, and stays searchable forever.

```
/analysis-init     # One-time setup (3 min)
/analysis-new      # Start analyzing
```

Works in **Claude Code** and **Cursor 2.4+**. Free and open source.

---

## ğŸ‘¥ Who is this for?

- **Data Analysts** â€” Full mode with 5 files, checklists, and quality gates for analyses that inform real decisions
- **PMs / Non-analysts** â€” Quick mode (single file) with guided questions so you can analyze without a statistics background
- **Growth / Marketing teams** â€” A/B test module, metric monitoring, impact tracking built in
- **Anyone using AI for analysis** â€” Stop losing insights in chat history

---

## â“ Why this exists

When you ask AI to "analyze this data," you get a one-shot answer. No structure, no tracking, no way to revisit your reasoning next month.

| Problem | How alive-analysis fixes it |
|---|---|
| Analysis is ad-hoc and different every time | Structured ALIVE loop, same quality every time |
| Insights vanish in chat history | Git-tracked markdown files, searchable archive |
| Easy to skip important checks | Stage checklists catch confounders, counter-metrics, sensitivity |
| Hard to share reasoning with your team | Audience-specific messaging, versioned documents |

---

## ğŸ§  The ALIVE Loop

Every analysis follows five stages. The AI doesn't generate answers â€” it **asks you questions** at each stage to structure your thinking.

```
ASK â†’ LOOK â†’ INVESTIGATE â†’ VOICE â†’ EVOLVE
 ?      ğŸ‘€       ğŸ”          ğŸ“¢      ğŸŒ±
```

- **ASK** â€” Define the question. Scope, hypothesis tree, success criteria.
- **LOOK** â€” Observe data. Quality checks, segmentation, confounders.
- **INVESTIGATE** â€” Analyze. Hypotheses, testing, multi-lens framework.
- **VOICE** â€” Communicate. "So what â†’ Now what" for each audience.
- **EVOLVE** â€” Next questions. Follow-ups, impact tracking, reflection.

---

## âš¡ Quick Demo

```
1. /analysis-init --quick        # Set language, team, mode
2. /analysis-new                 # "Why did D7 retention drop?"
3. AI asks questions â†’ you answer â†’ ASK stage done
4. /analysis-next                # Move through LOOK â†’ INVESTIGATE â†’ VOICE â†’ EVOLVE
5. /analysis-archive             # Done. File saved, searchable, Git-tracked.
```

The AI guides every step â€” you bring the domain knowledge, it brings the structure.

---

## ğŸ“„ Example Output

A completed Quick analysis looks like this:

```markdown
# Quick Investigation â€” Signup Rate Comparison
> ID: Q-2026-0212-001 | Type: Comparison | Status: Archived

## ASK
"Onboarding flow A vs B â€” which has higher signup completion?"

## LOOK
| Segment  | Flow A | Flow B | Users (A/B)    |
|----------|--------|--------|----------------|
| Organic  | 34%    | 41%    | 3,200 / 2,800  |
| Paid     | 28%    | 32%    | 1,500 / 1,200  |

## INVESTIGATE
Flow B outperforms A in every segment (+6-7pp).
No Simpson's Paradox. Drop-off at step 3 â€” Flow B made phone verification optional.

## VOICE
Ship Flow B. Monitor D7 activation as counter-metric.
Confidence: ğŸŸ¢ High (organic), ğŸŸ¡ Medium (paid â€” smaller sample)

## EVOLVE
Follow-up: Does simpler signup affect user quality? Check D30 activation.
```

See [`core/examples/`](core/examples/) for Full and Quick samples.

---

## âš™ï¸ Quick Start

### Install

```bash
git clone https://github.com/with-geun/alive-analysis.git /tmp/alive-analysis

bash /tmp/alive-analysis/install.sh            # Claude Code (default)
bash /tmp/alive-analysis/install.sh --cursor   # Cursor only
bash /tmp/alive-analysis/install.sh --both     # Both platforms
```

See [INSTALL.md](INSTALL.md) for manual setup.

### Start

Open your project in Claude Code or Cursor, then type in the **agent chat** (not the terminal):

```
/analysis-init            # Full setup (10 questions) or --quick (3 questions)
/analysis-new             # Start your first analysis
```

---

## âœ¨ Core Features

**16 commands** across analysis, experiments, monitoring, and modeling:

### Analysis (9 commands)
`/analysis-init` Â· `/analysis-new` Â· `/analysis-status` Â· `/analysis-next` Â· `/analysis-archive` Â· `/analysis-list` Â· `/analysis-promote` Â· `/analysis-search` Â· `/analysis-retro`

### Experiments (3 commands)
`/experiment-new` Â· `/experiment-next` Â· `/experiment-archive`

### Monitoring (3 commands)
`/monitor-setup` Â· `/monitor-check` Â· `/monitor-list`

### Modeling (1 command)
`/model-register`

**Key capabilities:**

- **Full & Quick modes** â€” 5-file deep analysis or single-file fast turnaround. Quick auto-promotes to Full when complexity grows.
- **3 analysis types** â€” Investigation ("why did X happen?"), Modeling ("can we predict Y?"), Simulation ("what if Z?")
- **A/B test experiments** â€” Design â†’ Validate â†’ Analyze â†’ Decide â†’ Learn. Pre-registration, SRM checks, guardrail metrics.
- **Metric monitoring** â€” 4-tier classification (North Star â†’ Leading â†’ Guardrail â†’ Diagnostic). Auto-escalation on consecutive warnings.
- **Insight search** â€” Full-text search across all analyses. Cross-reference analysis, conflicting finding detection, learning suggestions.
- **Auto retrospectives** â€” Period-based reports with impact outcomes, recurring patterns, and unresolved follow-ups.
- **Impact tracking** â€” Recommendation â†’ Decision â†’ Execution â†’ Result. Know if your analyses actually changed anything.
- **Tags & model registry** â€” Connect related analyses. Track ML model versions with drift monitoring.

### Platform support

| | Claude Code | Cursor 2.4+ |
|---|---|---|
| Interaction | Conversational (one question at a time) | Batch (all questions at once) |
| State | Session memory | File-based (`.analysis/status.md`) |
| SKILL.md | ~1,660 lines | ~265 lines |

Both platforms share the same `core/` methodology and produce identical outputs. `SKILL.md` is an [open standard](https://github.com/anthropics/claude-code) â€” works with any agent that supports it.

---

## ğŸ§© What this is NOT

- **Not a BI dashboard** â€” No charts or visualizations. It structures your *thinking*, not your *reporting*.
- **Not a statistics library** â€” It doesn't run models or crunch numbers. You bring the data, it brings the process.
- **Not AI doing analysis for you** â€” The AI asks questions and enforces structure. You make the analytical judgments.

---

## ğŸ“Š How teams use it

- **Growth team**: Quick analysis on metric drops â†’ finds root cause in one session â†’ archives with action items
- **PM**: `/analysis-new` Quick mode to investigate a feature hypothesis before writing a spec
- **Data team**: Full analysis for board-level decisions â†’ checklists ensure nothing is missed â†’ Impact Tracking proves ROI
- **Cross-functional**: PMs do Quick analyses independently, escalate to analysts for Full when needed

---

## ğŸ—ºï¸ Roadmap

- **v1.0** âœ… â€” ALIVE loop, Full/Quick modes, 3 analysis types, experiments, monitoring, search, retrospectives, dual-platform
- **Next** â€” Team dashboard

---

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md). Feedback on checklists and methodology especially welcome.

---

## â­ If this resonates

If alive-analysis helps you think more clearly about data, consider giving it a star. It helps others find the project.

---

## ğŸ“œ License

MIT

---

**Glossary**: New to data analysis terms? See [GLOSSARY.md](GLOSSARY.md).
**Language**: Works in any language â€” set yours during `/analysis-init`.
